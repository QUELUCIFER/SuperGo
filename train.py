from lib import game, mcts
import multiprocessing
from models import feature, value, policy
from const import *
import pickle
import timeit
import click

from multiprocessing import set_start_method
try:
    set_start_method('spawn')
except RuntimeError:
    pass


class Player:

    def __init__(self):
        if CUDA:
            self.extractor = feature.Extractor(INPLANES, OUTPLANES_MAP).cuda()
            self.value_net = value.ValueNet(OUTPLANES_MAP).cuda()
            self.policy_net = policy.PolicyNet(OUTPLANES_MAP, OUTPLANES).cuda()
        else:
            self.extractor = feature.Extractor(INPLANES, OUTPLANES_MAP)
            self.value_net = value.ValueNet(OUTPLANES_MAP)
            self.policy_net = policy.PolicyNet(OUTPLANES_MAP, OUTPLANES)    
        self.mcts = mcts.MCTS(C_PUCT, self.extractor, self.value_net, self.policy_net)
        self.passed = False


def create_dataset(player, opponent):
    """
    Used to create a learning dataset for the value and policy network.
    Play against itself and backtrack the winner to maximize winner moves
    probabilities
    """

    queue = multiprocessing.JoinableQueue()
    dataset = multiprocessing.Queue()
    train_dataset = []

    game_managers = [
        game.GameManager(queue, dataset)
        for _ in range(CPU_CORES)
    ]

    for game_manager in game_managers:
        game_manager.start()

    for id in range(SELF_PLAY_MATCH):
        queue.put(game.Game(player, opponent, id))
    
    for _ in range(CPU_CORES):
        queue.put(None)
    
    queue.join()
    
    for _ in range(SELF_PLAY_MATCH):
        result = dataset.get()
        train_dataset.append(pickle.loads(result))
    
    queue.close()
    return train_dataset


def train_epoch(player, optimizers, epoch, state, label):
    """ Used to train the 3 models over a single epoch """

    optimizers['value_opt'].zero_grad()
    optimizers['policy_opt'].zero_grad()



def train(dataset, player):
    """ Train the models using the data generated by the self-play """

    ## Add the weights of the feature extractor to both the 
    ## policy and the value so they get optimized by both loss
    joint_params_value = list(player.extractor.parameters()) + \
                         list(player.value_net.parameters())
    joint_params_policy = list(player.extractor.parameters()) + \
                         list(player.policy_net.parameters())

    value_optimizer = torch.optim.Adam(joint_params_value, lr=LR)
    policy_optimizer = torch.optim.Adam(joint_params_policy, lr=LR)
    optimizers = {
        'value_opt': value_optimizer,
        'policy_opt': policy_optimizer
    }

    epoch = 0
    for state, probas, label in dataset:
        example = {
            'state': state,
        }
        train_epoch(player, optimizers, epoch, example)



@click.command()
def main():
    player = Player()
    start_time = timeit.default_timer()
    dataset = create_dataset(player, player)
    total_time = timeit.default_timer() - start_time
    print('Dataset ready in: ---- %.3f seconds for %d self-play games ----' % (
        total_time, SELF_PLAY_MATCH))
    print('Average time per game: ---- %.3f seconds with %d processes ----' %
             (total_time / SELF_PLAY_MATCH, CPU_CORES))


if __name__ == "__main__":
    main()