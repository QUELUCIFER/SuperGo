import torch
from torch.autograd import Variable
from const import *


def train_epoch(player, optimizers, example):
    """ Used to train the 3 models over a single batch """

    optimizers['value_opt'].zero_grad()
    optimizers['policy_opt'].zero_grad()

    feature_maps = player.extractor(example['state'])
    winner = player.value_net(feature_maps)
    proba = player.policy_net(feature_maps)

    value_loss = player.value_net.criterion(winner, example['winner'])
    policy_loss = player.policy_net.criterion(proba, example['move'].view(-1))

    if example['winner'].data[0] == -1:
        policy_loss = 1 - policy_loss 

    value_loss.backward(retain_graph=True)
    policy_loss.backward()

    optimizers['value_opt'].step()
    optimizers['policy_opt'].step()



def train(dataloader, player):
    """ Train the models using the data generated by the self-play """

    ## Add the weights of the feature extractor to both the 
    ## policy and the value so they get optimized by both loss
    joint_params_value = list(player.extractor.parameters()) + \
                         list(player.value_net.parameters())
    joint_params_policy = list(player.extractor.parameters()) + \
                         list(player.policy_net.parameters())

    value_optimizer = torch.optim.Adam(joint_params_value, lr=LR)
    policy_optimizer = torch.optim.Adam(joint_params_policy, lr=LR)
    optimizers = {
        'value_opt': value_optimizer,
        'policy_opt': policy_optimizer
    }

    for batch_idx, (state, move, winner) in enumerate(dataloader):
        if batch_idx % 1000 == 0:
            print("batch index: %d" % (batch_idx / 1000))
        example = {
            'state': Variable(state).type(DTYPE),
            'move': Variable(move).type(DTYPE),
            'winner': Variable(winner).type(DTYPE)
        }
        train_epoch(player, optimizers, example)
    
    return player
