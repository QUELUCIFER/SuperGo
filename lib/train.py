import torch
import torch.nn as nn 
import numpy as np
import pickle
import time
import torch.nn.functional as F
import multiprocessing
import multiprocessing.pool
from .process import MyPool
from copy import deepcopy
from pymongo import MongoClient
from torch.autograd import Variable
from const import *
from models.agent import Player
from .dataset import SelfPlayDataset
from torch.utils.data import DataLoader
from .evaluate import evaluate
from lib.utils import load_player


class AlphaLoss(torch.nn.Module):
    """ Custom loss as defined in the paper """

    def __init__(self):
        super(AlphaLoss, self).__init__()
        self.log_softmax = nn.LogSoftmax()

    # def forward(self, winner, self_play_winner, probas, self_play_probas):
    #     value_error = F.mse_loss(winner, self_play_winner)
    #     policy_error = torch.mean(torch.sum(-self_play_probas * self.log_softmax(probas), 1))
    #     return value_error + policy_error
        
    def forward(self, winner, self_play_winner, probas, self_play_probas):
        value_error = F.mse_loss(winner, self_play_winner)
        policy_error = F.kl_div(probas, self_play_probas)
        return value_error + policy_error


def fetch_new_games(collection, dataset, last_id):
    """ Update the dataset with new games from the databse """

    ## Fetch new games in reverse order so we add the newest games first
    new_games = collection.find({"id": {"$gt": last_id}}).sort('_id', -1)
    added_moves = 0
    added_games = 0
    print("[TRAIN] Fetching: %d new games from the db"% (new_games.count()))

    for game in new_games:
        number_moves = dataset.update(pickle.loads(game['game']))
        added_moves += number_moves
        added_games += 1

        ## You cant replace more than 40% of the dataset at a time
        if added_moves > MOVES * 0.4:
            break
    
    print("[TRAIN] Last id: %d, added games: %d, added moves: %d"\
                    % (last_id, added_games, added_moves))
    return last_id + new_games.count()


def train_epoch(player, optimizer, example, criterion):
    """ Used to train the 3 models over a single batch """

    optimizer.zero_grad()

    feature_maps = player.extractor(example['state'])
    winner = player.value_net(feature_maps)
    probas = player.policy_net(feature_maps)

    loss = criterion(winner.view(-1), example['winner'], probas, example['move'])
    loss.backward()
    optimizer.step()

    return loss


def update_lr(lr, total_ite, lr_decay=LR_DECAY, lr_decay_ite=LR_DECAY_ITE):
    """ Decay learning rate by a factor of lr_decay every lr_decay_ite iteration """

    if total_ite % lr_decay_ite != 0 or lr <= 0.0001:
        return lr
    
    print("[TRAIN] Decaying the learning rate !")
    return lr * lr_decay


def train(current_time, loaded_ite):
    """ Train the models using the data generated by the self-play """

    last_id = 0
    ite = 1
    total_ite = 1
    lr = LR
    improvements = 1
    update = True
    pool = False
    criterion = AlphaLoss()
    dataset = SelfPlayDataset()
    
    ## Database connection
    client = MongoClient()
    collection = client.superGo[current_time]

    ## First player
    if loaded_ite:
        best_player, improvements = load_player(current_time, loaded_ite) 
        improvements -= 1
        last_id = collection.find().count() - 50
    else:
        best_player = Player()
        best_player.save_models(improvements, current_time)

    ## Callback after the evaluation is done
    def new_agent(result):
        if result:
            nonlocal best_player, update, improvements, \
                     pending_player, current_time
            best_player = pending_player
            update = True
            improvements += 1
        else:
            nonlocal last_id
            last_id = fetch_new_games(collection, dataset, last_id)

    ## Wait before the circular before is full
    while len(dataset) < MOVES:
        last_id = fetch_new_games(collection, dataset, last_id)
        time.sleep(5)

    print("[TRAIN] Circular buffer full !")
    print("[TRAIN] Starting to train !")
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)

    while True:
        if update:
            if improvements > 1 and total_ite != 1:
                best_player.save_models(improvements, current_time)
                print("[EVALUATION] New model has been saved")
            new_player = deepcopy(best_player)
            joint_params = list(new_player.extractor.parameters()) + \
                        list(new_player.policy_net.parameters()) +\
                        list(new_player.value_net.parameters())
            if ADAM:
                optimizer = torch.optim.Adam(joint_params, lr=lr, weight_decay=L2_REG)
            else:
                optimizer = torch.optim.SGD(joint_params, lr=lr, \
                                                weight_decay=L2_REG, momentum=MOMENTUM)
            ite = 1
            update = False
    
        for batch_idx, (state, move, winner) in enumerate(dataloader):

            ## Force the network to stop training the current network
            ## since the new one is better (from the callback)
            if update:
                break

            lr = update_lr(lr, total_ite)
            if ite % TRAIN_STEPS == 0:
                pending_player = deepcopy(new_player)
                last_id = fetch_new_games(collection, dataset, last_id)
                if pool:
                    pool.close()
                    pool.join()
                pool = MyPool(1)
                try:
                    pool.apply_async(evaluate, args=(best_player, pending_player,), \
                            callback=new_agent)
                except KeyboardInterrupt:
                    client.close()
                    pool.terminate()
            
            example = {
                'state': Variable(state).type(DTYPE_FLOAT),
                'winner': Variable(winner).type(DTYPE_FLOAT),
                'move' : Variable(move).type(DTYPE_FLOAT)
            }
            loss = train_epoch(new_player, optimizer, example, criterion)

            if ite % 10 == 0:
                print("[TRAIN] batch index: %d loss: %.3f"\
                        % (batch_idx / 10, loss))
            
            ite += 1
            total_ite += 1
    